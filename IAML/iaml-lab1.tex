\documentclass{article}

\usepackage{times}
\usepackage{hyperref}

\begin{document}

\title{IAML Practical 1}

\maketitle

\paragraph{What You Will Learn in This Lab}

\begin{enumerate}
\item How to construct simple classifiers using scikit-learn in python---Naive Bayes and Decision Trees 
\item Why making an incorrect assumption that all features are independent makes it easy to fool classifiers. 
\item How to evaluate the performance of a classifier: precision, recall, F-Score as metrics. Cross-validation 
\end{enumerate} 

To prepare for the lab, start iPython and decide who will be Student 1 and who will be Student 2. Student 1 will start off Part 1 (Naive Bayes), and Student 2 will start off Part 2 (Decision Trees).\marginpar{\small{Students should also use a unique code to tag their versions of files, if necessary.}}

You will need three data sets for this lab, \texttt{spambase}, \texttt{spambase$\_$test}, and \texttt{credit}.
\marginpar{\small{Note: This presumes that for each pair of students, a svn folder has been set up with the subfolders data, lab0, lab1, lab2, and lab3, and that all data sets are in the data folder. Lab 0 is the familiarisation lab.}}

Establish a connection, and tell each other what machine you are using, from where you are connecting, what type of connection you have, and by when you need to leave. This is especially important if it varies from the last time. Start up iPython, and then share your screen with each other (first Student 1, then Student 2). 

All instructions are for the coder, unless otherwise noted. 

\section{Naive Bayes: Spam Detection and Incorrect Independence Assumptions}

\subsubsection*{Coder: Student 1, Observer: Student 2}

The data set we will consider is the Spambase set, consisting of tagged emails from a single email account. Read through the description available for this data set to get a feel for what you're dealing with. If there are any terms that you do not understand, ask your tutor. Hint: Check that you understand what word frequencies mean.

First, import sklearn and numpy as np. 

Next, load the data using np.loadtxt from spambase.data and the header with feature names from the file spambase.header.\marginpar{\small{Add hint here.}}

\paragraph{Observer:} \emph{Look up the readline function in python if S1 nees help reading in the header, and check the syntax of numpy.loadtxt}

Create a markdown cell that describes the target classes and lists the number of features and the number of instances in the data set. Also list the index of the last feature that contains a word frequency and the index of the last feature that includes a character frequency.

Now, split the data set into the last column (target) and the predictor columns. Use only predictor columns that represent word frequencies. 

We are now ready to classify the data. Add a Markdown cell where you explain that classification is next.

The  sklearn.naive$\_$bayes contains everything you need to train Naive Bayes classifiers for various distributions. We will use the Gaussian distribution for now. Create an instance of a Gaussian Naive Bayes Classifier (GaussianNB), train it using the fit() method, and check its predictions on the training data using the predict() method. 

\paragraph{Observer:} Consult the scikit$\_$learn documentation 

For evaluating classifier performance, you can use the classification\_report() function from the metrics library. Use print to view the output. Finally, find and display the prior probabilities for each class using the attribute class\_prior\_. 

Now check in your notebook. 

\paragraph{Discussion:} Can you come up with a reason for the good performance? What would be the main practical problems we would face if we were not to make this assumption for this particular dataset?\\
Which words do you think might characterise non-spam emails particularly well? Note down 2--4 candidates. Write them down in a markdown cell. 
\marginpar{\small{This discussion point `How long did your classifer take to train and classify? Given this, how scalable do you think the Naïve Bayes classifier is to large datasets? Can you come up with a good reason for this?' does not work for scikit\_learn on a MacBook Air, because it is very fast.}}


\subsubsection*{Handover: Coder: S2, Observer: S1}
\paragraph{Observer:} \emph{Check out S1's notebook and run it. How long does it take to train the classifier?}

Determine the average frequency of each of the candidates you identified earlier for both ham and spam and add the information to the markdown cell using a table. 


\subsubsection*{In parallel:}

For the final part of this section we will now pretend we are spammers wishing to fool a spam checking system based on Naïve Bayes into classifying a spam e-mail as ham (i.e. a valid e-mail). 

In spambaseTest.data, you can find an example of an email that was classified as spam. Make a copy of the data set using your ID (spambaseTestID.data) and modify it to make the email look more like ``ham''. 

\paragraph{Observer:} \emph{For the following steps, save your current notebook in a separate file with your ID. We will be keeping the notebook edited by Student 2 as our main notebook for the moment, but we don't want to lose any of your notes.}

Load both the original test data and  your modified data using np.loadtext and split into target and predictors as above. Add a markdown cell where you describe your modifications. Predict the category of both. Has the class label (spam/non-spam) for this e-mail changed?

\paragraph{Discussion:} Compare the performance of your changes. Who did best, and why? If one of you didn't manage to persuade your classifier that the email is a non-spam email, try again. Use the method predict\_proba to check the probability that your new data point is spam versus ham. 

You've now managed to switch the predicted class label for that e-mail. Adding more ``hammy'' words to this e-mail has sufficiently increased the probability that this e-mail is ham so the classifier now outputs ``ham" as the e-mail's class label (by changing the word content of the e-mail you have added extra evidence or ``votes'' towards this e-mail being classified as ham). This is the ``stuffing'' example given in the lectures and is directly caused by the independence assumption that is made by Naive Bayes. Each word contributes independently of each other to the final score. This is a reason that a lot of spam e-mails include random excerpts from the passages of books so as to effectively add ``hammy'' words in the hope that the spam e-mail will bypass the spam filters. For this reason, in practice, many commercial e-mail systems (consider Gmail) likely use a lot more sophisticated spam detection models.

\section{Decision Trees} 

\subsubsection*{S2 Coder, S1 Observer}

One of the great advantages of decision trees is their interpretability. The rules learnt for classification are easy for a person to follow, unlike the opaque ``black box'' of many other methods, such as neural networks. We demonstrate the utility of this using a German credit data set. You can read a description of this dataset at the UCI site. The task is to predict whether a loan approval is good or bad credit risk based on 20 attributes. We've simplified the data set somewhat, particularly making attribute names and values more meaningful.\marginpar{\small{Is there a step by step documentation so that I can recreate this easily as a csv file from the original data.}}

Load the data set and provide a description similar to the one for spambase in a markdown box. 

When presented with a dataset, it is usually a good idea to visualise it first. Create a scatter plot  for age and duration. Do you notice anything unusual? Use the plot function from matplotlib.pyplot to produce the scatterplot. 

\paragraph{Observer:} \emph{Check Lab 0 for instructions on how to plot.}

\paragraph{Discussion:} The plot should have shown you a data point which seems to be corrupted, as some of its values are nonsensical. Even a single point like this can significantly affect the performance of a classifier. How do you think it would affect Decision trees? How about Naive Bayes? 

A good way to check this is to test the performance of each classifier before and after removing this datapoint.

Find the row which corresponds to the corrupted instance, and create a new instance of both your predictor and your target data without the invalid data point. Plot age versus duration again to make sure you have eliminated it. Once you're satisfied that you have found the correct data point, make a note in a markdown cell to explain why this one was eliminated.

\paragraph{Observer:} \emph{Read up on the python functions for finding all rows with a specific value in a given column in an array.}

Let's now train a decision tree. The corresponding sklearn library is called sklearn.tree, and you want the class DecisionTree; DecisionTreeClassifier() instantiates an object. First, train the decision tree as before using the fit() method and look how well it fits the training data using the predict() method. 

After training the classifier, we're plotting the decision tree. Import the library pydot2 and import the function export\_graphviz from sklearn.tree. 

\paragraph{Observer:} Read up on the syntax of export\_graphviz here: \url{http://scikit-learn.org/stable/modules/tree.html}


Such a tree accentuates one of the strengths of decision tree algorithms: they produce classifiers which are understandable to humans. This can be an important asset in real life applications (people are seldom prepared to do what a computer program tells them if there is no clear explanation). 

Next, evaluate its performance using metrics.classification\_report. In addition, look at the confusion matrix using metrics.confusion\_matrix which shows you how often data items from class are classified correctly, and how often they are misclassified. Report your findings in a markdown cell. 

\paragraph{Observer:} \emph{Check the syntax of the function for producing a confusion matrix.}

\paragraph{Discussion:} How would you assess the performance of the classifier? Is accuracy a sufficient measure in this case? Why? 
Looking at the decision tree itself, are the rules it applies sensible? Are there any branches which appear absurd? At what depth of the tree? What does this suggest? 
Hint: Check the rules applied after following the paths: (a) CheckingAccount = $<0$, Foreign = yes, Duration $>11$, Job = skilled, OtherDebtors = none, Duration $<=$ 30 and (b) CheckingAccount = $<0$, Foreign = yes, Duration $>11$, Job = unskilled. How does the decision tree deal with classification in the case where there are zero instances in the training set corresponding to that particular path in the tree?


Now, let's use 10-fold cross-validation. You will need the library cross\_validation from sklearn for this. Here is where our habit of instantiating a classifier before training it comes in handy, as the function for computing cross validated scores, cross\_validation.cross\_val\_scores, takes such a classifier instance as its argument. You can specify the number of folds using the parameter cv. Determine the mean and standard deviation of the scores. Create a markdown cell where you document both values. 

\paragraph{Observer:} Check the syntax on \url{http://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics}

\section{If you have time}

Use a different classification method on each data set, i.e., Naive Bayes on the credit data, and decision trees on spambase. Divide the labour, so that each of you looks at a single data set. Which classifier outperforms the other? What is the difference when we use cross validation? 



\begin{appendix}
	
	\section{About Spambase}\label{app:spambase}
	
	For more information, see file `spambase.DOCUMENTATION' at the
	UCI Machine Learning Repository: \url{http://www.ics.uci.edu/~mlearn/MLRepository.html}
	
	\paragraph{class:} nominal {0,1} class attribute of type spam
	= denotes whether the e-mail was considered spam (1) or not (0), 
	i.e. unsolicited commercial e-mail.  
	\paragraph{word\_freq\_WORD:} 
    48 continuous real [0,100] attributes
	= percentage of words in the e-mail that match WORD,
	i.e. 100 * (number of times the WORD appears in the e-mail) / 
	total number of words in e-mail.  A ``word'' in this case is any 
	string of alphanumeric characters bounded by non-alphanumeric 
	characters or end-of-string.
	
	\paragraph{char\_freq\_CHAR:} 6 continuous real [0,100] attributes 
	= percentage of characters in the e-mail that match CHAR,
	i.e. 100 * (number of CHAR occurences) / total characters in e-mail
	
\end{appendix}
	
	
	

	

\end{document}